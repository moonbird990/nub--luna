from fastapi import FastAPI, Request, Form
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import random
import time

app = FastAPI()
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

# Available models
MODELS = {
    "llama3-70b": "Meta-Llama-3-70B",
    "openchat": "OpenChat-3.5"
}

# Sample responses for demonstration
SAMPLE_RESPONSES = [
    "The essence of existence lies in the pursuit of knowledge.",
    "Quantum computing represents the next frontier in computational power.",
    "Human consciousness is a complex emergent property of neural networks.",
    "The universe's expansion is accelerating due to dark energy.",
    "Artificial intelligence will transform every aspect of human society.",
    "Nub Luna represents the unknown frontiers of artificial consciousness.",
    "The boundaries between human and machine intelligence continue to blur.",
    "Deep learning architectures are inspired by the human brain's neural networks."
]

@app.get("/")
async def home(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/ask")
async def ask_question(question: str = Form(...)):
    # Simulate model selection (in a real app, use your model selection logic)
    if len(question.split()) > 5 or '?' in question:
        selected_model = "llama3-70b"
    else:
        selected_model = "openchat"
    
    # Simulate response generation
    time.sleep(1.5)  # Simulate processing time
    response = f"{random.choice(SAMPLE_RESPONSES)} This response was generated by {MODELS[selected_model]}."
    
    return {
        "response": response,
        "model": selected_model
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)